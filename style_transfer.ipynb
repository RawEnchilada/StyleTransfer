{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b3XwN9V1nvR"
   },
   "source": [
    "# Neural style transfer\n",
    "\n",
    "This notebook is a documented learning experience in the topic of style transfer ai and neural networks.\n",
    "\n",
    "The sources can be found at the bottom of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Style Transfer\n",
    "\n",
    "Style Transfering is the reprezentation of a given image using another image's art style as reference.\n",
    "Here we have an example of my cat, stylized by an [existing style transer service](https://reiinakano.com/arbitrary-image-stylization-tfjs/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs:<br/>\n",
    "<img src=\".md/cat_input.jpg\" alt=\"cat\" width=\"200\"/>\n",
    "<img src=\".md/style_input.jpg\" alt=\"style\" width=\"200\"/>\n",
    "\n",
    "Result:<br/>\n",
    "<img src=\".md/result.jpg\" alt=\"result\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following chapters I will be attempting to recreate and improve upon this example using a neural style transer network created with [TensorFlow](https://www.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8ajP_u73s6m"
   },
   "source": [
    "## Setup\n",
    "\n",
    "### Imports and some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:25:10.086629Z",
     "iopub.status.busy": "2022-09-27T01:25:10.085844Z",
     "iopub.status.idle": "2022-09-27T01:25:10.675303Z",
     "shell.execute_reply": "2022-09-27T01:25:10.674485Z"
    },
    "id": "sc1OLbOWhPCO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "# Load compressed models from tensorflow_hub\n",
    "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n",
    "\n",
    "#To display images in the notebook\n",
    "import IPython.display as display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (12, 12)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import time\n",
    "import functools\n",
    "\n",
    "# Create an image from a tensor\n",
    "def tensor_to_image(tensor):\n",
    "  tensor = tensor*255\n",
    "  tensor = np.array(tensor, dtype=np.uint8)\n",
    "  if np.ndim(tensor)>3:\n",
    "    assert tensor.shape[0] == 1\n",
    "    tensor = tensor[0]\n",
    "  return PIL.Image.fromarray(tensor)\n",
    "\n",
    "# Define a function to load an image and limit its maximum dimension to 512 pixels.\n",
    "def load_img(path_to_img):\n",
    "  max_dim = 512\n",
    "  img = tf.io.read_file(path_to_img)\n",
    "  img = tf.image.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "  long_dim = max(shape)\n",
    "  scale = max_dim / long_dim\n",
    "\n",
    "  new_shape = tf.cast(shape * scale, tf.int32)\n",
    "\n",
    "  img = tf.image.resize(img, new_shape)\n",
    "  img = img[tf.newaxis, :]\n",
    "  return img\n",
    "\n",
    "# Display image in the notebook\n",
    "def imshow(image, title=None):\n",
    "  if len(image.shape) > 3:\n",
    "    image = tf.squeeze(image, axis=0)\n",
    "\n",
    "  plt.imshow(image)\n",
    "  if title:\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEwZ7FlwrjoZ"
   },
   "source": [
    "### VGG19\n",
    "Style transfer requires 2 neural networks, a trained feature extractor (also known as image classifier), and a transfer network.\n",
    "\n",
    "The [guide](https://www.tensorflow.org/tutorials/generative/style_transfer) I'm following, recommends using a [VGG19](https://keras.io/api/applications/vgg/#vgg19-function) model for feature extraction. Which is an image classification model of 19 layers, trained with thousands of images.\n",
    "\n",
    "I will load the model and show the result of the classification of this image:\n",
    "\n",
    "<img src=\".md/cat_input.jpg\" width=\"200\"/>\n",
    "\n",
    "The output will be a list of recognized objects in the image, if the pre-trained classifier model is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:25:25.574438Z",
     "iopub.status.busy": "2022-09-27T01:25:25.573998Z",
     "iopub.status.idle": "2022-09-27T01:25:30.571830Z",
     "shell.execute_reply": "2022-09-27T01:25:30.571052Z"
    },
    "id": "fMbzrr7BCTq0"
   },
   "outputs": [],
   "source": [
    "content_image = load_img('.md/cat_input.jpg')\n",
    "\n",
    "x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
    "x = tf.image.resize(x, (224, 224)) # VGG19 requires images to be 224x224\n",
    "vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
    "prediction_probabilities = vgg(x)\n",
    "prediction_probabilities.shape\n",
    "\n",
    "predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]\n",
    "[(class_name, prob) for (number, class_name, prob) in predicted_top_5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljpoYk-0f6HS"
   },
   "source": [
    "### Define content and style\n",
    "\n",
    "As I previously mentioned, the [VGG19](https://keras.io/api/applications/vgg/#vgg19-function) model consists of 19 layers. These layers represent basic and composited features of the image in the following manner:<br>\n",
    "The first few layers represent basic features like curves, edges, textures.<br>\n",
    "As you look deeper in the network, the layers represent more and more complicated features of the image, like eyes, ears, or even a face.\n",
    "\n",
    "Our model has the following layers:\n",
    " - input_4\n",
    " - block1_conv1\n",
    " - block1_conv2\n",
    " - block1_pool\n",
    " - block2_conv1\n",
    " - block2_conv2\n",
    " - block2_pool\n",
    " - block3_conv1\n",
    " - block3_conv2\n",
    " - block3_conv3\n",
    " - block3_conv4\n",
    " - block3_pool\n",
    " - block4_conv1\n",
    " - block4_conv2\n",
    " - block4_conv3\n",
    " - block4_conv4\n",
    " - block4_pool\n",
    " - block5_conv1\n",
    " - block5_conv2\n",
    " - block5_conv3\n",
    " - block5_conv4\n",
    " - block5_pool\n",
    "\n",
    "Since all these layers represent different features in the image, we can define which should be used for style and which for content. This allow the network to extract only the defining features of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:25:31.477654Z",
     "iopub.status.busy": "2022-09-27T01:25:31.477005Z",
     "iopub.status.idle": "2022-09-27T01:25:31.481086Z",
     "shell.execute_reply": "2022-09-27T01:25:31.480426Z"
    },
    "id": "ArfX_6iA0WAX"
   },
   "outputs": [],
   "source": [
    "content_layers = ['block5_conv2'] \n",
    "\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1', \n",
    "                'block4_conv1', \n",
    "                'block5_conv1']\n",
    "\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt3i3RRrJiOX"
   },
   "source": [
    "## Build the model \n",
    "\n",
    "Our pre-trained classifier model defines the input and output layers for us, so all we need to do is supply a `keras.Model` with them. Here we define a function which creates the keras Model using only the layers we specify.\n",
    "\n",
    "Then we can use this model to get the values of the layers for a specific image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:25:31.485357Z",
     "iopub.status.busy": "2022-09-27T01:25:31.484683Z",
     "iopub.status.idle": "2022-09-27T01:25:31.489975Z",
     "shell.execute_reply": "2022-09-27T01:25:31.489299Z"
    },
    "id": "nfec6MuMAbPx"
   },
   "outputs": [],
   "source": [
    "def vgg_layers(layer_names):\n",
    "\n",
    "  # Load our model. Load pretrained VGG, trained on ImageNet data\n",
    "  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "  vgg.trainable = False\n",
    "  \n",
    "  outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "\n",
    "  model = tf.keras.Model([vgg.input], outputs)\n",
    "  return model\n",
    "\n",
    "style_image = load_img('.md/style_input.jpg')\n",
    "\n",
    "style_extractor = vgg_layers(style_layers)\n",
    "style_outputs = style_extractor(style_image*255)\n",
    "\n",
    "#Look at the statistics of each layer's output\n",
    "for name, output in zip(style_layers, style_outputs):\n",
    "  print(name)\n",
    "  print(\"  shape: \", output.numpy().shape)\n",
    "  print(\"  min: \", output.numpy().min())\n",
    "  print(\"  max: \", output.numpy().max())\n",
    "  print(\"  mean: \", output.numpy().mean())\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGUfttK9F8d5"
   },
   "source": [
    "These values can be used to calculate a [Gram Matrix](https://en.wikipedia.org/wiki/Gram_matrix) for each layer, which can represent the layer's style.\n",
    "\n",
    "We use the following function for calculating a Gram Matrix:\n",
    "\n",
    "$$G^l_{cd} = \\frac{\\sum_{ij} F^l_{ijc}(x)F^l_{ijd}(x)}{IJ}$$\n",
    "\n",
    "which can be implemented concisely using the `tf.linalg.einsum` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:25:32.306933Z",
     "iopub.status.busy": "2022-09-27T01:25:32.306225Z",
     "iopub.status.idle": "2022-09-27T01:25:32.310525Z",
     "shell.execute_reply": "2022-09-27T01:25:32.309904Z"
    },
    "id": "HAy1iGPdoEpZ"
   },
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "  input_shape = tf.shape(input_tensor)\n",
    "  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
    "  return result/(num_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue from here -------------- TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXIUX6czZABh"
   },
   "source": [
    "## Extract style and content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HGHvwlJ1nkn"
   },
   "source": [
    "Build a model that returns the style and content tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:25:32.314725Z",
     "iopub.status.busy": "2022-09-27T01:25:32.314126Z",
     "iopub.status.idle": "2022-09-27T01:25:32.320730Z",
     "shell.execute_reply": "2022-09-27T01:25:32.320041Z"
    },
    "id": "Sr6QALY-I1ja"
   },
   "outputs": [],
   "source": [
    "class StyleContentModel(tf.keras.models.Model):\n",
    "  def __init__(self, style_layers, content_layers):\n",
    "    super(StyleContentModel, self).__init__()\n",
    "    self.vgg = vgg_layers(style_layers + content_layers)\n",
    "    self.style_layers = style_layers\n",
    "    self.content_layers = content_layers\n",
    "    self.num_style_layers = len(style_layers)\n",
    "    self.vgg.trainable = False\n",
    "\n",
    "  def call(self, inputs):\n",
    "    \"Expects float input in [0,1]\"\n",
    "    inputs = inputs*255.0\n",
    "    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
    "    outputs = self.vgg(preprocessed_input)\n",
    "    style_outputs, content_outputs = (outputs[:self.num_style_layers],\n",
    "                                      outputs[self.num_style_layers:])\n",
    "\n",
    "    style_outputs = [gram_matrix(style_output)\n",
    "                     for style_output in style_outputs]\n",
    "\n",
    "    content_dict = {content_name: value\n",
    "                    for content_name, value\n",
    "                    in zip(self.content_layers, content_outputs)}\n",
    "\n",
    "    style_dict = {style_name: value\n",
    "                  for style_name, value\n",
    "                  in zip(self.style_layers, style_outputs)}\n",
    "\n",
    "    return {'content': content_dict, 'style': style_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xuj1o33t1edl"
   },
   "source": [
    "When called on an image, this model returns the gram matrix (style) of the `style_layers` and content of the `content_layers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:25:32.324552Z",
     "iopub.status.busy": "2022-09-27T01:25:32.323870Z",
     "iopub.status.idle": "2022-09-27T01:25:33.018606Z",
     "shell.execute_reply": "2022-09-27T01:25:33.017851Z"
    },
    "id": "rkjO-DoNDU0A"
   },
   "outputs": [],
   "source": [
    "extractor = StyleContentModel(style_layers, content_layers)\n",
    "\n",
    "results = extractor(tf.constant(content_image))\n",
    "\n",
    "print('Styles:')\n",
    "for name, output in sorted(results['style'].items()):\n",
    "  print(\"  \", name)\n",
    "  print(\"    shape: \", output.numpy().shape)\n",
    "  print(\"    min: \", output.numpy().min())\n",
    "  print(\"    max: \", output.numpy().max())\n",
    "  print(\"    mean: \", output.numpy().mean())\n",
    "  print()\n",
    "\n",
    "print(\"Contents:\")\n",
    "for name, output in sorted(results['content'].items()):\n",
    "  print(\"  \", name)\n",
    "  print(\"    shape: \", output.numpy().shape)\n",
    "  print(\"    min: \", output.numpy().min())\n",
    "  print(\"    max: \", output.numpy().max())\n",
    "  print(\"    mean: \", output.numpy().mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9r8Lyjb_m0u"
   },
   "source": [
    "## Run gradient descent\n",
    "\n",
    "With this style and content extractor, you can now implement the style transfer algorithm. Do this by calculating the mean square error for your image's output relative to each target, then take the weighted sum of these losses.\n",
    "\n",
    "Set your style and content target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:25:33.022429Z",
     "iopub.status.busy": "2022-09-27T01:25:33.022149Z",
     "iopub.status.idle": "2022-09-27T01:25:33.073015Z",
     "shell.execute_reply": "2022-09-27T01:25:33.072201Z"
    },
    "id": "PgkNOnGUFcKa"
   },
   "outputs": [],
   "source": [
    "style_targets = extractor(style_image)['style']\n",
    "content_targets = extractor(content_image)['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNPrpl-e_w9A"
   },
   "source": [
    "Define a `tf.Variable` to contain the image to optimize. To make this quick, initialize it with the content image (the `tf.Variable` must be the same shape as the content image):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:25:33.077603Z",
     "iopub.status.busy": "2022-09-27T01:25:33.076923Z",
     "iopub.status.idle": "2022-09-27T01:25:33.083075Z",
     "shell.execute_reply": "2022-09-27T01:25:33.082452Z"
    },
    "id": "J0vKxF8ZO6G8"
   },
   "outputs": [],
   "source": [
    "image = tf.Variable(content_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6L8ojmn_6rH"
   },
   "source": [
    "Since this is a float image, define a function to keep the pixel values between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:25:33.086936Z",
     "iopub.status.busy": "2022-09-27T01:25:33.086328Z",
     "iopub.status.idle": "2022-09-27T01:25:33.089911Z",
     "shell.execute_reply": "2022-09-27T01:25:33.089308Z"
    },
    "id": "kdgpTJwL_vE2"
   },
   "outputs": [],
   "source": [
    "def clip_0_1(image):\n",
    "  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBU5RFpcAo7W"
   },
   "source": [
    "Create an optimizer. The paper recommends LBFGS, but Adam works okay, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:25:33.093603Z",
     "iopub.status.busy": "2022-09-27T01:25:33.093007Z",
     "iopub.status.idle": "2022-09-27T01:25:33.096937Z",
     "shell.execute_reply": "2022-09-27T01:25:33.096279Z"
    },
    "id": "r4XZjqUk_5Eu"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "As-evbBiA2qT"
   },
   "source": [
    "To optimize this, use a weighted combination of the two losses to get the total loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:25:33.100432Z",
     "iopub.status.busy": "2022-09-27T01:25:33.099876Z",
     "iopub.status.idle": "2022-09-27T01:25:33.103247Z",
     "shell.execute_reply": "2022-09-27T01:25:33.102434Z"
    },
    "id": "Dt4pxarvA4I4"
   },
   "outputs": [],
   "source": [
    "style_weight=1e-2\n",
    "content_weight=1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:25:33.106905Z",
     "iopub.status.busy": "2022-09-27T01:25:33.106377Z",
     "iopub.status.idle": "2022-09-27T01:25:33.111514Z",
     "shell.execute_reply": "2022-09-27T01:25:33.110861Z"
    },
    "id": "0ggx2Na8oROH"
   },
   "outputs": [],
   "source": [
    "def style_content_loss(outputs):\n",
    "    style_outputs = outputs['style']\n",
    "    content_outputs = outputs['content']\n",
    "    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n",
    "                           for name in style_outputs.keys()])\n",
    "    style_loss *= style_weight / num_style_layers\n",
    "\n",
    "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n",
    "                             for name in content_outputs.keys()])\n",
    "    content_loss *= content_weight / num_content_layers\n",
    "    loss = style_loss + content_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbF2WnP9BI5M"
   },
   "source": [
    "Use `tf.GradientTape` to update the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:25:33.115056Z",
     "iopub.status.busy": "2022-09-27T01:25:33.114626Z",
     "iopub.status.idle": "2022-09-27T01:25:33.118977Z",
     "shell.execute_reply": "2022-09-27T01:25:33.118296Z"
    },
    "id": "0t0umkajFIuh"
   },
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(image):\n",
    "  with tf.GradientTape() as tape:\n",
    "    outputs = extractor(image)\n",
    "    loss = style_content_loss(outputs)\n",
    "\n",
    "  grad = tape.gradient(loss, image)\n",
    "  opt.apply_gradients([(grad, image)])\n",
    "  image.assign(clip_0_1(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FHMJq4UBRIQ"
   },
   "source": [
    "Now run a few steps to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:25:33.122755Z",
     "iopub.status.busy": "2022-09-27T01:25:33.122183Z",
     "iopub.status.idle": "2022-09-27T01:25:36.176020Z",
     "shell.execute_reply": "2022-09-27T01:25:36.175319Z"
    },
    "id": "Y542mxi-O2a2"
   },
   "outputs": [],
   "source": [
    "train_step(image)\n",
    "train_step(image)\n",
    "train_step(image)\n",
    "tensor_to_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNzE-mTbBVgY"
   },
   "source": [
    "Since it's working, perform a longer optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:25:36.186026Z",
     "iopub.status.busy": "2022-09-27T01:25:36.185271Z",
     "iopub.status.idle": "2022-09-27T01:26:11.447924Z",
     "shell.execute_reply": "2022-09-27T01:26:11.447177Z"
    },
    "id": "rQW1tXYoLbUS"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "epochs = 10\n",
    "steps_per_epoch = 100\n",
    "\n",
    "step = 0\n",
    "for n in range(epochs):\n",
    "  for m in range(steps_per_epoch):\n",
    "    step += 1\n",
    "    train_step(image)\n",
    "    print(\".\", end='', flush=True)\n",
    "  display.clear_output(wait=True)\n",
    "  display.display(tensor_to_image(image))\n",
    "  print(\"Train step: {}\".format(step))\n",
    "  \n",
    "end = time.time()\n",
    "print(\"Total time: {:.1f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWVB3anJMY2v"
   },
   "source": [
    "## Total variation loss\n",
    "\n",
    "One downside to this basic implementation is that it produces a lot of high frequency artifacts. Decrease these using an explicit regularization term on the high frequency components of the image. In style transfer, this is often called the *total variation loss*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:26:11.459141Z",
     "iopub.status.busy": "2022-09-27T01:26:11.458473Z",
     "iopub.status.idle": "2022-09-27T01:26:11.462864Z",
     "shell.execute_reply": "2022-09-27T01:26:11.461985Z"
    },
    "id": "7szUUybCQMB3"
   },
   "outputs": [],
   "source": [
    "def high_pass_x_y(image):\n",
    "  x_var = image[:, :, 1:, :] - image[:, :, :-1, :]\n",
    "  y_var = image[:, 1:, :, :] - image[:, :-1, :, :]\n",
    "\n",
    "  return x_var, y_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:26:11.466269Z",
     "iopub.status.busy": "2022-09-27T01:26:11.465646Z",
     "iopub.status.idle": "2022-09-27T01:26:12.536873Z",
     "shell.execute_reply": "2022-09-27T01:26:12.535901Z"
    },
    "id": "Atc2oL29PXu_"
   },
   "outputs": [],
   "source": [
    "x_deltas, y_deltas = high_pass_x_y(content_image)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: Original\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: Original\")\n",
    "\n",
    "x_deltas, y_deltas = high_pass_x_y(image)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: Styled\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: Styled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqHElVgBkgkz"
   },
   "source": [
    "This shows how the high frequency components have increased.\n",
    "\n",
    "Also, this high frequency component is basically an edge-detector. You can get similar output from the Sobel edge detector, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:26:12.566210Z",
     "iopub.status.busy": "2022-09-27T01:26:12.565398Z",
     "iopub.status.idle": "2022-09-27T01:26:13.225145Z",
     "shell.execute_reply": "2022-09-27T01:26:13.224258Z"
    },
    "id": "HyvqCiywiUfL"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "sobel = tf.image.sobel_edges(content_image)\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(clip_0_1(sobel[..., 0]/4+0.5), \"Horizontal Sobel-edges\")\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(clip_0_1(sobel[..., 1]/4+0.5), \"Vertical Sobel-edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vv5bKlSDnPP7"
   },
   "source": [
    "The regularization loss associated with this is the sum of the squares of the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:26:13.238956Z",
     "iopub.status.busy": "2022-09-27T01:26:13.238669Z",
     "iopub.status.idle": "2022-09-27T01:26:13.242655Z",
     "shell.execute_reply": "2022-09-27T01:26:13.241979Z"
    },
    "id": "mP-92lXMIYPn"
   },
   "outputs": [],
   "source": [
    "def total_variation_loss(image):\n",
    "  x_deltas, y_deltas = high_pass_x_y(image)\n",
    "  return tf.reduce_sum(tf.abs(x_deltas)) + tf.reduce_sum(tf.abs(y_deltas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:26:13.246051Z",
     "iopub.status.busy": "2022-09-27T01:26:13.245505Z",
     "iopub.status.idle": "2022-09-27T01:26:13.255844Z",
     "shell.execute_reply": "2022-09-27T01:26:13.255168Z"
    },
    "id": "s4OYBUX2KQ25"
   },
   "outputs": [],
   "source": [
    "total_variation_loss(image).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pu2hJ8zOKMc1"
   },
   "source": [
    "That demonstrated what it does. But there's no need to implement it yourself, TensorFlow includes a standard implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:26:13.259689Z",
     "iopub.status.busy": "2022-09-27T01:26:13.259076Z",
     "iopub.status.idle": "2022-09-27T01:26:13.267023Z",
     "shell.execute_reply": "2022-09-27T01:26:13.266391Z"
    },
    "id": "YQjWW04NKLfJ"
   },
   "outputs": [],
   "source": [
    "tf.image.total_variation(image).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTessd-DCdcC"
   },
   "source": [
    "## Re-run the optimization\n",
    "\n",
    "Choose a weight for the `total_variation_loss`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:26:13.270291Z",
     "iopub.status.busy": "2022-09-27T01:26:13.270041Z",
     "iopub.status.idle": "2022-09-27T01:26:13.273230Z",
     "shell.execute_reply": "2022-09-27T01:26:13.272538Z"
    },
    "id": "tGeRLD4GoAd4"
   },
   "outputs": [],
   "source": [
    "total_variation_weight=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kG1-T4kJsoAv"
   },
   "source": [
    "Now include it in the `train_step` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:26:13.276982Z",
     "iopub.status.busy": "2022-09-27T01:26:13.276333Z",
     "iopub.status.idle": "2022-09-27T01:26:13.292764Z",
     "shell.execute_reply": "2022-09-27T01:26:13.292097Z"
    },
    "id": "BzmfcyyYUyWq"
   },
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(image):\n",
    "  with tf.GradientTape() as tape:\n",
    "    outputs = extractor(image)\n",
    "    loss = style_content_loss(outputs)\n",
    "    loss += total_variation_weight*tf.image.total_variation(image)\n",
    "\n",
    "  grad = tape.gradient(loss, image)\n",
    "  opt.apply_gradients([(grad, image)])\n",
    "  image.assign(clip_0_1(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcLWBQChsutQ"
   },
   "source": [
    "Reinitialize the image-variable and the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:26:13.296487Z",
     "iopub.status.busy": "2022-09-27T01:26:13.295975Z",
     "iopub.status.idle": "2022-09-27T01:26:13.300519Z",
     "shell.execute_reply": "2022-09-27T01:26:13.299883Z"
    },
    "id": "a-dPRr8BqexB"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
    "image = tf.Variable(content_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEflRstmtGBu"
   },
   "source": [
    "And run the optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T01:26:13.303858Z",
     "iopub.status.busy": "2022-09-27T01:26:13.303434Z",
     "iopub.status.idle": "2022-09-27T01:26:49.879419Z",
     "shell.execute_reply": "2022-09-27T01:26:49.878636Z"
    },
    "id": "q3Cc3bLtoOWy"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "epochs = 10\n",
    "steps_per_epoch = 100\n",
    "\n",
    "step = 0\n",
    "for n in range(epochs):\n",
    "  for m in range(steps_per_epoch):\n",
    "    step += 1\n",
    "    train_step(image)\n",
    "    print(\".\", end='', flush=True)\n",
    "  display.clear_output(wait=True)\n",
    "  display.display(tensor_to_image(image))\n",
    "  print(\"Train step: {}\".format(step))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total time: {:.1f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNlwRXagxQZk"
   },
   "source": [
    "# References\n",
    "These are the pages I've used to learn about the subject.\n",
    " - https://www.tensorflow.org/tutorials/generative/style_transfer\n",
    " - https://keras.io/api/\n",
    " - https://arxiv.org/pdf/1409.1556.pdf\n",
    " - https://www.fritz.ai/style-transfer/\n",
    " - https://en.wikipedia.org/wiki/Gram_matrix"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "style_transfer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "5312a05d4c1062e7e99dc4a1327331a73c17135ec320657c0e8e8843948bfa85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
